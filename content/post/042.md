---
title: "第042期 一键部署私有大语言模型：CPU & GPU加速教程，附安全方案"
date: 2025-04-09T09:16:00+08:00
draft: false
tags: ["本地部署","私有大语言模型","Ollama","Open WebUI","Docker","CPU","GPU","甲骨文服务器","Vultr","HTTPS","反向代理","Caddy","Linux哲学","容器编排","TLS证书","VPS主机"]
categories: ["视频"]
author: "最初的晨曦"

contentCopyright: '<a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">本文章采用 CC BY-NC-SA 4.0 许可协议</a>'
---

![](../../images/042/0.jpg)
	
## 视频连接
- [YouTube](https://youtu.be/uJrmcohW_X0)
- [B站](https://www.bilibili.com/video/BV13LdPYyE54/)
- [西瓜视频](https://www.ixigua.com/7491191737210146088)

## 本期视频内容

大家好！老王归来，带来一期硬核技术分享！本期视频将教你如何在本地部署私有的大语言模型，彻底掌控自己的AI工具。我们将对比传统的复杂安装流程与全新的一键Docker部署方案，让你轻松实现CPU和GPU加速。无论是甲骨文服务器还是Vultr GPU主机，都能完美运行！此外，我们还解决了公网部署的安全问题，带来现代化的HTTPS反向代理方案，确保你的服务稳健又安全。如果你想快速上手并体验强大的AI模型，这期视频绝对不容错过！

## 相关代码

1. Open WebUI 与 Ollama 捆绑安装(仅使用 CPU)

```shell
docker run -d -p 3000:8080 \
-v ./ollama:/root/.ollama \
-v ./open-webui:/app/backend/data \
--name open-webui \
--restart always ghcr.io/open-webui/open-webui:ollama
```

2. Open WebUI 与 Ollama 捆绑安装(启用 GPU 支持)

```shell
docker run -d -p 3000:8080 \
-v ./ollama:/root/.ollama \
-v ./open-webui:/app/backend/data \
--gpus all \
--name open-webui \
--restart always ghcr.io/open-webui/open-webui:ollama

# 检查显卡驱动
sudo docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

[VULTR 按小时计费，支持GPU的VPS](https://www.vultr.com/?ref=9742814)

3. caddy反代

Caddyfile
```
# 域名先配置dns解析
test.104300.xyz {
    reverse_proxy 127.0.0.1:3000
	# 这里换成你的email
    tls test@youremail.com
}
```

命令
```shell
docker run -d \
  --name caddy2 \
  --restart always \
  --network host \
  -v ./caddy2/Caddyfile:/etc/caddy/Caddyfile \
  -v ./caddy2/caddy_data:/data \
  -v ./caddy2/caddy_config:/config \
  -v /etc/localtime:/etc/localtime:ro \
  caddy:alpine
```

4. docker-compose.yml

```yml
services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      - $PWD/ollama:/root/.ollama
    # 如果使用GPU，把下面取消注释
    # deploy:
      # resources:
        # reservations:
          # devices:
            # - driver: nvidia
              # count: all
              # capabilities: [gpu]
    restart: always

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - $PWD/open-webui:/app/backend/data
    restart: always
    depends_on:
      - ollama

  caddy2:
    image: caddy:alpine
    container_name: caddy2
    restart: always
    cap_add:
      - NET_ADMIN
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    volumes:
      - $PWD/caddy2/Caddyfile:/etc/caddy/Caddyfile
      - $PWD/caddy2/caddy_data:/data
      - $PWD/caddy2/caddy_config:/config
      - /etc/localtime:/etc/localtime:ro
    depends_on:
      - open-webui

```

---

VPS推荐：[VULTR](https://www.vultr.com/?ref=9742814)&nbsp;&nbsp;&nbsp;&nbsp;[搬瓦工](https://bwh81.net/aff.php?aff=73687)  
自用机场：[https://d.126126.xyz/3](https://d.126126.xyz/3)  
youtube：[https://www.youtube.com/c/ericwang618](https://www.youtube.com/c/ericwang618)  
哔哩哔哩：[https://space.bilibili.com/221010336](https://space.bilibili.com/221010336)  
Telegram：[https://t.me/first_sunlight](https://t.me/first_sunlight)  
个人网站：[https://www.126126.xyz](https://www.126126.xyz)  
合作邮箱：fs104300@outlook.com